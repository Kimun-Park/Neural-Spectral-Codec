# Training Configuration for Neural Spectral Codec GNN

# Inherits from default.yaml and overrides/adds training-specific parameters

# Training hyperparameters
training:
  # Optimizer
  optimizer: "adam"  # Optimizer type (adam, sgd, adamw)
  learning_rate: 5.0e-4  # Initial learning rate
  weight_decay: 1.0e-5  # L2 regularization
  momentum: 0.9  # Momentum for SGD (if used)

  # Learning rate schedule
  lr_scheduler: "step"  # LR scheduler (step, cosine, plateau)
  lr_decay_epochs: [30, 40]  # Epochs to decay LR
  lr_decay_factor: 0.1  # LR decay factor
  min_lr: 1.0e-6  # Minimum learning rate

  # Training duration
  n_epochs: 50  # Total number of epochs
  batch_size: 32  # Batch size for triplet mining (not used for graph)

  # Early stopping
  early_stopping: true  # Enable early stopping
  patience: 10  # Patience for early stopping (epochs)

  # Gradient settings
  grad_clip: 1.0  # Gradient clipping threshold (null to disable)

# Triplet loss settings
triplet:
  margin: 0.1  # Triplet loss margin

  # Positive pair criteria
  positive_distance_max: 5.0  # Max distance for positive pairs (meters)
  positive_temporal_min: 30  # Min temporal gap for positives (frames)

  # Negative pair criteria (hard negative mining)
  negative_distance_min: 10.0  # Min distance for negatives (meters)
  negative_distance_max: 50.0  # Max distance for negatives (meters)

  # Mining strategy
  mining_strategy: "hard"  # Mining strategy (hard, semi-hard, random)
  n_negatives_per_anchor: 1  # Number of negatives per anchor

  # Sampling
  sample_rate: 1.0  # Fraction of triplets to use per epoch

# Data augmentation (optional for future enhancement)
augmentation:
  enabled: false  # Enable data augmentation
  rotation_range: 0.0  # Random rotation in radians (currently disabled for rotation invariance test)
  translation_range: 0.0  # Random translation in meters
  intensity_jitter: 0.0  # Intensity jitter factor

# Validation settings
validation:
  interval: 1  # Validate every N epochs
  compute_recall: true  # Compute Recall@K metrics
  recall_k_values: [1, 5, 10, 20]  # K values for Recall@K
  recall_distance_threshold: 5.0  # Distance threshold for recall (meters)

# Checkpoint settings
checkpoint:
  save_best: true  # Save best model based on validation
  save_last: true  # Always save last checkpoint
  metric: "recall@1"  # Metric for best model (recall@1, loss)
  mode: "max"  # Mode for best model (max, min)

# Resume training
resume:
  enabled: false  # Resume from checkpoint
  checkpoint_path: null  # Path to checkpoint (if resuming)
  load_optimizer: true  # Load optimizer state

# W&B specific settings for training
wandb:
  enabled: false  # Enable W&B logging for this run
  watch_model: true  # Watch model gradients
  log_gradients: true  # Log gradient histograms
  log_frequency: 100  # Log frequency (iterations)

# GPU settings
gpu:
  mixed_precision: false  # Use mixed precision training (AMP)
  compile: false  # Use torch.compile (PyTorch 2.0+)
  cudnn_benchmark: true  # Enable cuDNN benchmark for faster training

# Ablation study flags
ablation:
  disable_gnn: false  # Disable GNN (use raw histograms only)
  disable_temporal_edges: false  # Disable temporal edges in graph
  disable_context: false  # Disable context injection in retrieval

# Expected performance targets (for validation)
targets:
  recall_at_1: 0.978  # Target Recall@1: 97.8%
  encoding_time_ms: 10  # Target encoding time: <10ms
  query_latency_ms: 27  # Target query latency: 27ms @ 100K database
  compression_ratio: 132  # Target compression: 132x vs Scan Context
